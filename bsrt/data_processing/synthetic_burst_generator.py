import math
import random
from dataclasses import dataclass, field
from typing import List, Tuple, TypedDict, Union

import cv2
import numpy as np
import numpy.typing as npt
import torch
from torch import Tensor
from torchvision.transforms import CenterCrop, ConvertImageDtype, RandomCrop

from bsrt.data_processing.camera_pipeline import (
    apply_ccm,
    gamma_expansion,
    invert_smoothstep,
    mosaic,
    random_ccm,
)
from bsrt.data_processing.image_processing_params import ImageProcessingParams
from bsrt.data_processing.image_transformation_params import ImageTransformationParams
from bsrt.data_processing.meta_info import MetaInfo
from bsrt.data_processing.noises import Noises
from bsrt.data_processing.rgb_gains import RgbGains
from bsrt.utils.data_format_utils import numpy_to_torch, torch_to_numpy
from bsrt.utils.types import InterpolationType

# TODO: We can't enable fullgraph because it causes too many recompiles.
# TODO: Find a clean way to know which device to allocate the tensors on.


def rgb2rawburst(
    image: Tensor,
    burst_size: int,
    downsample_factor: float = 1,
    burst_transformation_params: Union[ImageTransformationParams, None] = None,
    image_processing_params: Union[ImageProcessingParams, None] = None,
    interpolation_type: InterpolationType = "bilinear",
) -> Tuple[Tensor, Tensor, Tensor, Tensor, MetaInfo]:
    """Generates a synthetic LR RAW burst from the input image. The input sRGB image is first
    converted to linear sensor space using an inverse camera pipeline. A LR burst is then
    generated by applying random transformations defined by burst_transformation_params to the
    input image, and downsampling it by the downsample_factor. The generated burst is then
    mosaicekd and corrputed by random noise.
    """

    if image_processing_params is None:
        image_processing_params = ImageProcessingParams()

    # Sample camera pipeline params
    if image_processing_params.random_ccm:
        rgb2cam = random_ccm()
    else:
        rgb2cam = torch.eye(3).float()
    cam2rgb = rgb2cam.inverse()

    # Approximately inverts global tone mapping.
    if image_processing_params.smoothstep:
        image = invert_smoothstep(image)

    # Inverts gamma compression.
    if image_processing_params.compress_gamma:
        image = gamma_expansion(image)

    # Inverts color correction.
    image = apply_ccm(image, rgb2cam)

    # Sample gains
    # FIXME: This just makes the image VERY green.
    if image_processing_params.random_gains:
        # Approximately inverts white balance and brightening.
        gains = RgbGains.random_gains()
        image = gains.safe_invert_gains(image)
    else:
        gains = RgbGains(1.0, 1.0, 1.0)

    # Clip saturated pixels.
    image = image.clamp(0.0, 1.0)

    # Generate LR burst
    image_burst_rgb, flow_vectors = single2lrburst(
        image=image,
        burst_size=burst_size,
        downsample_factor=downsample_factor,
        transformation_params=burst_transformation_params,
        interpolation_type=interpolation_type,
    )

    # mosaic
    image_burst = mosaic(image_burst_rgb.clone())

    # Add noise
    if image_processing_params.add_noise:
        noises = Noises.random_noise_levels()
        image_burst = noises.apply(image_burst)
    else:
        noises = Noises(0.0, 0.0)

    # Clip saturated pixels.
    image_burst = image_burst.clamp(0.0, 1.0)

    meta_info = MetaInfo(
        rgb2cam=rgb2cam,
        cam2rgb=cam2rgb,
        gains=gains,
        smoothstep=image_processing_params.smoothstep,
        compress_gamma=image_processing_params.compress_gamma,
        noises=noises,
    )

    return image_burst, image, image_burst_rgb, flow_vectors, meta_info


def get_tmat(
    image_shape: Tuple[int, int],
    translation: Tuple[float, float],
    theta: float,
    shear_values: Tuple[float, float],
    scale_factors: Tuple[float, float],
) -> npt.NDArray[np.float64]:
    """Generates a transformation matrix corresponding to the input transformation parameters"""
    im_h, im_w = image_shape

    t_mat = np.identity(3)

    t_mat[0, 2] = translation[0]
    t_mat[1, 2] = translation[1]
    t_rot = cv2.getRotationMatrix2D((im_w * 0.5, im_h * 0.5), theta, 1.0)
    t_rot = np.concatenate((t_rot, np.array([0.0, 0.0, 1.0]).reshape(1, 3)))

    t_shear = np.array(
        [
            [1.0, shear_values[0], -shear_values[0] * 0.5 * im_w],
            [shear_values[1], 1.0, -shear_values[1] * 0.5 * im_h],
            [0.0, 0.0, 1.0],
        ]
    )

    t_scale = np.array(
        [[scale_factors[0], 0.0, 0.0], [0.0, scale_factors[1], 0.0], [0.0, 0.0, 1.0]]
    )

    t_mat = t_scale @ t_rot @ t_shear @ t_mat

    t_mat = t_mat[:2, :]

    return t_mat


def numpy_get_tmat(
    image_shape: Tuple[int, int],
    translation: Tuple[float, float],
    theta: float,
    shear_values: Tuple[float, float],
    scale_factors: Tuple[float, float],
) -> npt.NDArray[np.float64]:
    """Generates a transformation matrix corresponding to the input transformation parameters"""
    # It may look suspect that we have image_y, image_x, but that's because the image shape is
    # (height, width) (row-major order)
    image_y, image_x = image_shape
    image_middle_x = image_x * 0.5
    image_middle_y = image_y * 0.5
    translate_x, translate_y = translation
    shear_x, shear_y = shear_values
    scale_x, scale_y = scale_factors
    # To match the semantics of OpenCV's getRotationMatrix2D, we rotate counter-clockwise by
    # negative theta.
    rad = np.deg2rad(-theta)

    translate = np.array(
        [[1.0, 0.0, translate_x], [0.0, 1.0, translate_y], [0.0, 0.0, 1.0]],
        dtype=np.float64,
    )

    # Shear using the given shear factors, about the center of the image.
    # See https://lectureloops.com/shear-transformation/ for more.
    shear = np.array(
        [
            [1.0, shear_x, -0.5 * shear_x * image_x],
            [shear_y, 1.0, -0.5 * shear_y * image_y],
            [0.0, 0.0, 1.0],
        ],
        dtype=np.float64,
    )

    # Translate the center to the origin
    shift_center_to_origin = np.array(
        [
            [1.0, 0.0, -image_middle_x],
            [0.0, 1.0, -image_middle_y],
            [0.0, 0.0, 1.0],
        ],
        dtype=np.float64,
    )

    # This matrix performs the rotation about the origin.
    rotate = np.array(
        [
            [np.cos(rad), -np.sin(rad), 0.0],
            [np.sin(rad), np.cos(rad), 0.0],
            [0.0, 0.0, 1.0],
        ],
        dtype=np.float64,
    )

    # Translate the center back to where it was
    unshift_center_to_origin = np.array(
        [
            [1.0, 0.0, image_middle_x],
            [0.0, 1.0, image_middle_y],
            [0.0, 0.0, 1.0],
        ],
        dtype=np.float64,
    )

    # Scale by scale factors
    scale = np.array(
        [[scale_x, 0.0, 0.0], [0.0, scale_y, 0.0], [0.0, 0.0, 1.0]],
        dtype=np.float64,
    )

    expected = (
        scale @ unshift_center_to_origin @ rotate @ shift_center_to_origin @ shear @ translate
    )
    expected = expected[:2, :]
    return expected


def numpy_fused_rotate_get_tmat(
    image_shape: Tuple[int, int],
    translation: Tuple[float, float],
    theta: float,
    shear_values: Tuple[float, float],
    scale_factors: Tuple[float, float],
) -> npt.NDArray[np.float64]:
    """Generates a transformation matrix corresponding to the input transformation parameters"""
    # It may look suspect that we have image_y, image_x, but that's because the image shape is
    # (height, width) (row-major order)
    image_y, image_x = image_shape
    image_middle_x = image_x * 0.5
    image_middle_y = image_y * 0.5
    translate_x, translate_y = translation
    shear_x, shear_y = shear_values
    scale_x, scale_y = scale_factors
    # To match the semantics of OpenCV's getRotationMatrix2D, we rotate counter-clockwise by
    # negative theta.
    rad = np.deg2rad(-theta)

    translate = np.array(
        [[1.0, 0.0, translate_x], [0.0, 1.0, translate_y], [0.0, 0.0, 1.0]],
        dtype=np.float64,
    )

    # Shear using the given shear factors, about the center of the image.
    # See https://lectureloops.com/shear-transformation/ for more.
    shear = np.array(
        [
            [1.0, shear_x, -0.5 * shear_x * image_x],
            [shear_y, 1.0, -0.5 * shear_y * image_y],
            [0.0, 0.0, 1.0],
        ],
        dtype=np.float64,
    )

    # This matrix performs the rotation about the center of the image. It is equivalent to
    # the following:
    #   1. Translate the center of the image to the origin
    #   2. Rotate the image about the origin
    #   3. Translate the center of the image back to where it was
    # It is unshift_center_to_origin @ rotate @ shift_center_to_origin from pure_python_get_tmat.
    rotate = np.array(
        [
            [
                np.cos(rad),
                -np.sin(rad),
                image_middle_x * (1 - np.cos(rad)) + image_middle_y * np.sin(rad),
            ],
            [
                np.sin(rad),
                np.cos(rad),
                image_middle_y * (1 - np.cos(rad)) - image_middle_x * np.sin(rad),
            ],
            [0.0, 0.0, 1.0],
        ],
        dtype=np.float64,
    )

    # Scale by scale factors
    scale = np.array(
        [[scale_x, 0.0, 0.0], [0.0, scale_y, 0.0], [0.0, 0.0, 1.0]],
        dtype=np.float64,
    )

    expected = scale @ rotate @ shear @ translate
    expected = expected[:2, :]
    return expected


def numpy_fused_scale_rotate_get_tmat(
    image_shape: Tuple[int, int],
    translation: Tuple[float, float],
    theta: float,
    shear_values: Tuple[float, float],
    scale_factors: Tuple[float, float],
) -> npt.NDArray[np.float64]:
    """Generates a transformation matrix corresponding to the input transformation parameters"""
    # It may look suspect that we have image_y, image_x, but that's because the image shape is
    # (height, width) (row-major order)
    image_y, image_x = image_shape
    image_middle_x = image_x * 0.5
    image_middle_y = image_y * 0.5
    translate_x, translate_y = translation
    shear_x, shear_y = shear_values
    scale_x, scale_y = scale_factors
    # To match the semantics of OpenCV's getRotationMatrix2D, we rotate counter-clockwise by
    # negative theta.
    rad = np.deg2rad(-theta)

    translate = np.array(
        [[1.0, 0.0, translate_x], [0.0, 1.0, translate_y], [0.0, 0.0, 1.0]],
        dtype=np.float64,
    )

    # Shear using the given shear factors, about the center of the image.
    # See https://lectureloops.com/shear-transformation/ for more.
    shear = np.array(
        [
            [1.0, shear_x, -0.5 * shear_x * image_x],
            [shear_y, 1.0, -0.5 * shear_y * image_y],
            [0.0, 0.0, 1.0],
        ],
        dtype=np.float64,
    )

    # This matrix performs the rotation about the center of the image and scales.
    # It is equivalent to the following:
    #   1. Translate the center of the image to the origin
    #   2. Rotate the image about the origin
    #   3. Translate the center of the image back to where it was
    #   4. Scale by scale factors
    # It is scale @ unshift_center_to_origin @ rotate @ shift_center_to_origin from
    # pure_python_get_tmat.
    scale_after_rotate = np.array(  # Rotate
        [
            [
                scale_x * np.cos(rad),
                -scale_x * np.sin(rad),
                scale_x * (image_middle_x * (1 - np.cos(rad)) + image_middle_y * np.sin(rad)),
            ],
            [
                scale_y * np.sin(rad),
                scale_y * np.cos(rad),
                scale_y * (image_middle_y * (1 - np.cos(rad)) - image_middle_x * np.sin(rad)),
            ],
            [0.0, 0.0, 1.0],
        ],
        dtype=np.float64,
    )

    expected = scale_after_rotate @ shear @ translate
    expected = expected[:2, :]
    return expected


def numpy_fused_scale_rotate_and_shear_translate_get_tmat(
    image_shape: Tuple[int, int],
    translation: Tuple[float, float],
    theta: float,
    shear_values: Tuple[float, float],
    scale_factors: Tuple[float, float],
) -> npt.NDArray[np.float64]:
    """Generates a transformation matrix corresponding to the input transformation parameters"""
    # It may look suspect that we have image_y, image_x, but that's because the image shape is
    # (height, width) (row-major order)
    image_y, image_x = image_shape
    image_middle_x = image_x * 0.5
    image_middle_y = image_y * 0.5
    translate_x, translate_y = translation
    shear_x, shear_y = shear_values
    scale_x, scale_y = scale_factors
    # To match the semantics of OpenCV's getRotationMatrix2D, we rotate counter-clockwise by
    # negative theta.
    rad = np.deg2rad(-theta)

    # Translate the image by the given translation factors. Then shears the image using the given
    # shear factors, about the center of the image.
    # See https://lectureloops.com/shear-transformation/ for more.
    # It is shear @ translate from pure_python_get_tmat.
    shear_after_translate = np.array(
        [
            [1.0, shear_x, shear_x * translate_y - 0.5 * shear_x * image_x + translate_x],
            [shear_y, 1.0, shear_y * translate_x - 0.5 * shear_y * image_y + translate_y],
            [0.0, 0.0, 1.0],
        ],
    )

    # This matrix performs the rotation about the center of the image and scales.
    # It is equivalent to the following:
    #   1. Translate the center of the image to the origin
    #   2. Rotate the image about the origin
    #   3. Translate the center of the image back to where it was
    #   4. Scale by scale factors
    # It is scale @ unshift_center_to_origin @ rotate @ shift_center_to_origin from
    # pure_python_get_tmat.
    scale_after_rotate = np.array(  # Rotate
        [
            [
                scale_x * np.cos(rad),
                -scale_x * np.sin(rad),
                scale_x * (image_middle_x * (1 - np.cos(rad)) + image_middle_y * np.sin(rad)),
            ],
            [
                scale_y * np.sin(rad),
                scale_y * np.cos(rad),
                scale_y * (image_middle_y * (1 - np.cos(rad)) - image_middle_x * np.sin(rad)),
            ],
            [0.0, 0.0, 1.0],
        ],
        dtype=np.float64,
    )

    expected = scale_after_rotate @ shear_after_translate
    expected = expected[:2, :]
    return expected


def get_translate_mat(
    translation: Tuple[float, float],
    dtype: torch.dtype = torch.float64,
    device: torch.device = torch.device("cpu"),
) -> torch.Tensor:
    """Generates a translation matrix corresponding to the input translation parameters"""
    translate_x, translate_y = translation

    return torch.tensor(
        [[1.0, 0.0, translate_x], [0.0, 1.0, translate_y], [0.0, 0.0, 1.0]],
        dtype=dtype,
        device=device,
    )


def get_shear_mat(
    image_shape: Tuple[int, int],
    shear_values: Tuple[float, float],
    dtype: torch.dtype = torch.float64,
    device: torch.device = torch.device("cpu"),
) -> torch.Tensor:
    """Generates a shear matrix corresponding to the input shear parameters"""
    image_y, image_x = image_shape
    shear_x, shear_y = shear_values

    return torch.tensor(
        [
            [1.0, shear_x, -0.5 * shear_x * image_x],
            [shear_y, 1.0, -0.5 * shear_y * image_y],
            [0.0, 0.0, 1.0],
        ],
        dtype=dtype,
        device=device,
    )


def get_rotate_mat(
    image_shape: Tuple[int, int],
    theta: float,
    dtype: torch.dtype = torch.float64,
    device: torch.device = torch.device("cpu"),
) -> torch.Tensor:
    """Generates a rotation matrix corresponding to the input rotation angle, around the center of
    the image"""
    image_y, image_x = image_shape
    image_middle_x = image_x * 0.5
    image_middle_y = image_y * 0.5
    rad = math.radians(-theta)
    sin_rad = math.sin(rad)
    cos_rad = math.cos(rad)

    return torch.tensor(
        [
            [
                cos_rad,
                -sin_rad,
                image_middle_x - image_middle_x * cos_rad + image_middle_y * sin_rad,
            ],
            [
                sin_rad,
                cos_rad,
                image_middle_y - image_middle_x * sin_rad - image_middle_y * cos_rad,
            ],
            [0.0, 0.0, 1.0],
        ],
        dtype=dtype,
        device=device,
    )


def get_scale_mat(
    scale_factors: Tuple[float, float],
    dtype: torch.dtype = torch.float64,
    device: torch.device = torch.device("cpu"),
) -> torch.Tensor:
    """Generates a scaling matrix corresponding to the input scaling factors"""
    scale_x, scale_y = scale_factors

    return torch.tensor(
        [[scale_x, 0.0, 0.0], [0.0, scale_y, 0.0], [0.0, 0.0, 1.0]],
        dtype=dtype,
        device=device,
    )


def torch_get_tmat(
    image_shape: Tuple[int, int],
    translation: Tuple[float, float],
    theta: float,
    shear_values: Tuple[float, float],
    scale_factors: Tuple[float, float],
    dtype: torch.dtype = torch.float64,
    device: torch.device = torch.device("cpu"),
) -> Tensor:
    """Generates a transformation matrix corresponding to the input transformation parameters"""
    translate_mat = get_translate_mat(translation, dtype, device)
    shear_mat = get_shear_mat(image_shape, shear_values, dtype, device)
    rotate_mat = get_rotate_mat(image_shape, theta, dtype, device)
    scale_mat = get_scale_mat(scale_factors, dtype, device)

    expected = scale_mat @ rotate_mat @ shear_mat @ translate_mat
    expected = expected[:2, :]
    return expected


def torch_fused_get_tmat1(
    image_shape: Tuple[int, int],
    translation: Tuple[float, float],
    theta: float,
    shear_values: Tuple[float, float],
    scale_factors: Tuple[float, float],
    dtype: torch.dtype = torch.float64,
    device: torch.device = torch.device("cpu"),
) -> Tensor:
    """Generates a transformation matrix corresponding to the input transformation parameters"""
    image_y, image_x = image_shape
    image_middle_x = image_x * 0.5
    image_middle_y = image_y * 0.5
    translate_x, translate_y = translation
    shear_x, shear_y = shear_values
    scale_x, scale_y = scale_factors
    rad = math.radians(-theta)
    sin_rad = math.sin(rad)
    cos_rad = math.cos(rad)

    # Translate the image by the given translation factors. Then shears the image using the given
    # shear factors, about the center of the image.
    # See https://lectureloops.com/shear-transformation/ for more.
    # It is shear @ translate from pure_python_get_tmat.
    shear_after_translate = torch.tensor(
        [
            [1.0, shear_x, shear_x * translate_y - 0.5 * shear_x * image_x + translate_x],
            [shear_y, 1.0, shear_y * translate_x - 0.5 * shear_y * image_y + translate_y],
            [0.0, 0.0, 1.0],
        ],
        dtype=dtype,
        device=device,
    )

    # This matrix performs the rotation about the center of the image and scales.
    # It is equivalent to the following:
    #   1. Translate the center of the image to the origin
    #   2. Rotate the image about the origin
    #   3. Translate the center of the image back to where it was
    #   4. Scale by scale factors
    # It is scale @ unshift_center_to_origin @ rotate @ shift_center_to_origin from
    # pure_python_get_tmat.
    scale_after_rotate = torch.tensor(
        [
            [
                scale_x * cos_rad,
                -scale_x * sin_rad,
                scale_x * (image_middle_x * (1 - cos_rad) + image_middle_y * sin_rad),
            ],
            [
                scale_y * sin_rad,
                scale_y * cos_rad,
                scale_y * (image_middle_y * (1 - cos_rad) - image_middle_x * sin_rad),
            ],
            [0.0, 0.0, 1.0],
        ],
        dtype=dtype,
        device=device,
    )

    expected = scale_after_rotate @ shear_after_translate
    expected = expected[:2, :]
    return expected


def single2lrburst(
    image: Union[Tensor, npt.NDArray[np.uint8]],
    burst_size: int,
    downsample_factor: float = 1.0,
    transformation_params: Union[ImageTransformationParams, None] = None,
    interpolation_type: InterpolationType = "bilinear",
) -> Tuple[Tensor, Tensor]:
    """Generates a burst of size burst_size from the input image by applying random
    transformations defined by transformation_params, and downsampling the resulting burst by
    downsample_factor.
    """
    if transformation_params is None:
        transformation_params = ImageTransformationParams()

    if interpolation_type == "nearest":
        interpolation = cv2.INTER_NEAREST
    elif interpolation_type == "bilinear":
        interpolation = cv2.INTER_LINEAR
    elif interpolation_type == "bicubic":
        interpolation = cv2.INTER_CUBIC
    elif interpolation_type == "lanczos":
        interpolation = cv2.INTER_LANCZOS4

    normalize = False
    if isinstance(image, torch.Tensor):
        if image.max() < 2.0:
            image = image * 255.0
            normalize = True
        image = torch_to_numpy(image).astype(np.uint8)

    burst: List[Tensor] = []
    sample_pos_inv_all: List[Tensor] = []

    rvs, cvs = torch.meshgrid(
        [torch.arange(0, image.shape[0]), torch.arange(0, image.shape[1])],
        indexing="ij",
    )

    sample_grid = torch.stack((cvs, rvs, torch.ones_like(cvs)), dim=-1).float()

    for i in range(burst_size):
        if i == 0:
            # For base image, do not apply any random transformations. We only translate the image
            # to center the sampling grid
            shift: float = (downsample_factor / 2.0) - 0.5
            translation: Tuple[float, float] = (shift, shift)
            theta: float = 0.0
            shear_factor: Tuple[float, float] = (0.0, 0.0)
            scale_factor_tuple: Tuple[float, float] = (1.0, 1.0)
        else:
            # Sample random image transformation parameters
            max_translation = transformation_params.max_translation

            if max_translation <= 0.01:
                shift = (downsample_factor / 2.0) - 0.5
                translation = (shift, shift)
            else:
                translation = (
                    random.uniform(-max_translation, max_translation),
                    random.uniform(-max_translation, max_translation),
                )

            max_rotation = transformation_params.max_rotation
            theta = random.uniform(-max_rotation, max_rotation)

            max_shear = transformation_params.max_shear
            shear_x = random.uniform(-max_shear, max_shear)
            shear_y = random.uniform(-max_shear, max_shear)
            shear_factor = (shear_x, shear_y)

            max_ar_factor = transformation_params.max_ar_factor
            ar_factor: float = np.exp(random.uniform(-max_ar_factor, max_ar_factor))

            max_scale = transformation_params.max_scale
            scale_factor: float = np.exp(random.uniform(-max_scale, max_scale))

            scale_factor_tuple: Tuple[float, float] = (
                scale_factor,
                scale_factor * ar_factor,
            )

        output_sz: Tuple[int, int] = (image.shape[1], image.shape[0])

        # Generate a affine transformation matrix corresponding to the sampled parameters
        t_mat: npt.NDArray[np.float64] = get_tmat(
            (image.shape[0], image.shape[1]),
            translation,
            theta,
            shear_factor,
            scale_factor_tuple,
        )
        t_mat_tensor: Tensor = torch.from_numpy(t_mat)

        # Apply the sampled affine transformation
        image_t: npt.NDArray[np.float64] = cv2.warpAffine(
            image, t_mat, output_sz, flags=interpolation, borderMode=cv2.BORDER_CONSTANT
        )

        t_mat_tensor_3x3: Tensor = torch.cat(
            (t_mat_tensor.float(), torch.tensor([0.0, 0.0, 1.0]).view(1, 3)), dim=0
        )
        t_mat_tensor_inverse: Tensor = t_mat_tensor_3x3.inverse()[:2, :].contiguous()

        sample_pos_inv: Tensor = torch.mm(
            sample_grid.view(-1, 3), t_mat_tensor_inverse.t().float()
        ).view(*sample_grid.shape[:2], -1)

        if transformation_params.border_crop > 0:
            border_crop = transformation_params.border_crop

            image_t = image_t[border_crop:-border_crop, border_crop:-border_crop, :]
            sample_pos_inv = sample_pos_inv[border_crop:-border_crop, border_crop:-border_crop, :]

        # Downsample the image
        image_t = cv2.resize(
            image_t,
            None,
            fx=1.0 / downsample_factor,
            fy=1.0 / downsample_factor,
            interpolation=interpolation,
        )
        sample_pos_inv = cv2.resize(
            sample_pos_inv.numpy(),
            None,
            fx=1.0 / downsample_factor,
            fy=1.0 / downsample_factor,
            interpolation=interpolation,
        )

        sample_pos_inv = torch.from_numpy(sample_pos_inv).permute(2, 0, 1).contiguous()

        if normalize:
            image_t_tensor = numpy_to_torch(image_t).float() / 255.0
        else:
            image_t_tensor = numpy_to_torch(image_t).float()
        burst.append(image_t_tensor)
        sample_pos_inv_all.append(sample_pos_inv / downsample_factor)

    burst_images = torch.stack(burst)
    sample_pos_inv_all_tensor = torch.stack(sample_pos_inv_all)

    # Compute the flow vectors to go from the i'th burst image to the base image
    flow_vectors = sample_pos_inv_all_tensor - sample_pos_inv_all_tensor[:, :1, ...]

    return burst_images, flow_vectors


class SyntheticBurstGeneratorData(TypedDict):
    """
    burst: Generated LR RAW burst, a torch tensor of shape

        [
            burst_size,
            4,
            self.crop_sz / (2*self.downsample_factor),
            self.crop_sz / (2*self.downsample_factor)
        ]

        The 4 channels correspond to 'R', 'G', 'G', and 'B' values in the RGGB bayer mosaick.
        The extra factor 2 in the denominator (2*self.downsample_factor) corresponds to the
        mosaicking operation.

    frame_gt: The HR RGB ground truth in the linear sensor space, a torch tensor of shape
        [3, self.crop_sz, self.crop_sz]

    flow_vectors: The ground truth flow vectors between a burst image and the base image (i.e. the
        first image in the burst).
        The flow_vectors can be used to warp the burst images to the base frame, using the 'warp'
        function in utils.warp package.
        flow_vectors is torch tensor of shape

        [
            burst_size,
            2,
            self.crop_sz / self.downsample_factor,
            self.crop_sz / self.downsample_factor
        ]

        Note that the flow_vectors are in the LR RGB space, before mosaicking. Hence it has twice
        the number of rows and columns, compared to the output burst.

        NOTE: The flow_vectors are only available during training for the purpose of using any
        auxiliary losses if needed. The flow_vectors will NOT be provided for the bursts in the
        test set

    meta_info: A dictionary containing the parameters used to generate the synthetic burst.
    """

    burst: Tensor
    gt: Tensor
    flow_vectors: Tensor
    # meta_info: MetaInfo


@dataclass(eq=False)
class SyntheticBurstGeneratorTransform(torch.nn.Module):
    """Synthetic burst dataset for joint denoising, demosaicking, and super-resolution. RAW Burst
    sequences are synthetically generated on the fly as follows. First, a single image is loaded
    from the base_dataset. The sampled image is converted to linear sensor space using the inverse
    camera pipeline employed in [1]. A burst sequence is then generated by adding random
    translations and rotations to the converted image. The generated burst is then converted is
    then mosaicked, and corrupted by random noise to obtain the RAW burst.

    [1] Unprocessing Images for Learned Raw Denoising, Brooks, Tim and Mildenhall, Ben and Xue,
    Tianfan and Chen, Jiawen and Sharlet, Dillon and Barron, Jonathan T, CVPR 2019
    """

    burst_size: int
    crop_sz: int
    dtype: torch.dtype
    final_crop_sz: int = field(init=False)
    downsample_factor: int = 4
    burst_transformation_params: ImageTransformationParams = ImageTransformationParams(
        max_translation=24.0,
        max_rotation=1.0,
        max_shear=0.0,
        max_scale=0.0,
        border_crop=24,
    )
    image_processing_params: ImageProcessingParams = ImageProcessingParams(
        random_ccm=False,
        random_gains=False,
        smoothstep=False,
        compress_gamma=False,
        add_noise=False,
    )
    interpolation_type: InterpolationType = "bilinear"

    def __post_init__(self):
        super().__init__()
        self.final_crop_sz = self.crop_sz + 2 * self.burst_transformation_params.border_crop
        self.cropper = RandomCrop(self.final_crop_sz)
        self.boundary_ignorer = CenterCrop(self.crop_sz)
        self.pre_dtype_converter = ConvertImageDtype(torch.float32)
        self.post_dtype_converter = ConvertImageDtype(self.dtype)

    def __call__(self, frame: Tensor) -> SyntheticBurstGeneratorData:
        # Extract a random crop from the image
        cropped_frame: Tensor = self.cropper(frame)
        converted_frame: Tensor = self.pre_dtype_converter(cropped_frame)

        burst, gt, _burst_rgb, flow_vectors, meta_info = rgb2rawburst(
            converted_frame,
            self.burst_size,
            self.downsample_factor,
            burst_transformation_params=self.burst_transformation_params,
            image_processing_params=self.image_processing_params,
            interpolation_type=self.interpolation_type,
        )
        burst = self.post_dtype_converter(burst)
        gt = self.post_dtype_converter(self.boundary_ignorer(gt))
        flow_vectors = self.post_dtype_converter(flow_vectors)

        return SyntheticBurstGeneratorData(
            burst=burst,
            gt=gt,
            flow_vectors=flow_vectors,
        )
